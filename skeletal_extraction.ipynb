{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Loading"
      ],
      "metadata": {
        "id": "hJPfi0wSQLn3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khWOT1-XhV-P",
        "outputId": "00dbb273-047c-4142-9226-a43386e7d370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.22.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.7.0.72)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.0 sounddevice-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odF19HHg7YzQ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class Video:\n",
        "  def __init__(self, path):\n",
        "    self.path = path\n",
        "    self.video = cv2.VideoCapture(self.path) \n",
        "    self.cur_frame = 0\n",
        "    self.fps = self.video.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "  def get_frame_id(self, minutes, seconds):\n",
        "    frame_id = int(self.fps*(minutes*60+seconds))\n",
        "    return frame_id\n",
        "\n",
        "  def set_frame(self, frame_id):\n",
        "    self.video.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
        "    self.cur_frame = frame_id\n",
        "\n",
        "  def get_frame(self, frame_id):\n",
        "    self.set_frame(frame_id)\n",
        "    success, frame = self.video.read()\n",
        "    return frame\n",
        "\n",
        "  def get_cur_frame(self):\n",
        "    success, frame = self.video.read()\n",
        "    return frame\n",
        "\n",
        "  def get_set_frame(self, minutes, seconds):\n",
        "    self.set_frame_time(minutes, seconds)\n",
        "    return self.get_cur_frame()\n",
        "\n",
        "  def get_timeframe(self, start_minutes, start_seconds, end_minutes, end_seconds, frame_rate):\n",
        "    n_frames = (self.get_frame_id(end_minutes, end_seconds) - self.get_frame_id(start_minutes, start_seconds)) // frame_rate\n",
        "    cur_frame_id = self.get_frame_id(start_minutes, start_seconds)\n",
        "    frame_list = []\n",
        "    for i in range(0, n_frames):\n",
        "      frame = self.get_frame(cur_frame_id)\n",
        "      frame_list.append(frame)\n",
        "      cur_frame_id += frame_rate\n",
        "    return frame_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Xv9eE4fDNJT"
      },
      "outputs": [],
      "source": [
        "DESIRED_HEIGHT = 480\n",
        "DESIRED_WIDTH = 480\n",
        "def resize_and_show(image):\n",
        "  h, w = image.shape[:2]\n",
        "  if h < w:\n",
        "    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
        "  else:\n",
        "    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
        "  cv2_imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Frame List"
      ],
      "metadata": {
        "id": "JOLa4NpHQk5P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzFkutywBjCo"
      },
      "outputs": [],
      "source": [
        "files = [\"americano_part3\", \"diggin_part2\", \"jagger_part4\", \"pink_part3\"] \n",
        "songs = ['americano', 'diggin', 'jagger', 'pink']\n",
        "\n",
        "for f, s in zip(files, songs):\n",
        "  vid = Video(\"/content/drive/MyDrive/Thesis/Experiment Dances/{}.mov\".format(f))\n",
        "  time_df = pd.read_csv ('/content/drive/MyDrive/Thesis/Experiment Scores/{}.csv'.format(s))\n",
        "\n",
        "  start_frames = []\n",
        "  end_frames = []\n",
        "\n",
        "  for index, row in time_df.iterrows():\n",
        "\n",
        "    start_time = row['start_time']\n",
        "    start_minute, start_second = start_time.split(':')\n",
        "    start_frame = vid.get_frame_id(int(start_minute), float(start_second))\n",
        "    start_frames.append(start_frame)\n",
        "\n",
        "    end_time = row['end_time']\n",
        "    end_minute, end_second = end_time.split(':')\n",
        "    end_frame = vid.get_frame_id(int(end_minute), float(end_second))\n",
        "    end_frames.append(end_frame)\n",
        "\n",
        "  time_df['start_frame'] = start_frames\n",
        "  time_df['end_frame'] = end_frames\n",
        "\n",
        "  time_df.to_csv('/content/drive/MyDrive/Thesis/Experiment Dances/{}_frames.csv'.format(f))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract SK Data"
      ],
      "metadata": {
        "id": "LvRjk5uXQrtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the skeletal data from all the particicpants for every song:"
      ],
      "metadata": {
        "id": "M3guXWnL0E40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_drawing = mp.solutions.drawing_utils \n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "frame_rate = 2\n",
        "failed_frames = []\n",
        "\n",
        "landmarks = [\"nose\", \"left eye (inner)\", \"left eye\", \"left eye (outer)\", \"right eye (inner)\", \"right eye\", \"right eye (outer)\", \"left ear\", \"right ear\",\n",
        "             \"mouth (left)\", \"mouth (right)\", \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\", \"left wrist\", \"right wrist\", \"left pinky\",\n",
        "             \"right pinky\", \"left index\", \"right index\", \"left thumb\", \"right thumb\", \"left hip\", \"right hip\", \"left knee\", \"right knee\", \"left ankle\",\n",
        "             \"right ankle\", \"left heel\", \"right heel\", \"left foot index\", \"right foot index\"]\n",
        "\n",
        "songs = [\"americano\", \"diggin\", \"jagger\", \"pink\"] \n",
        "participants = [\"part1\", \"part2\", \"part3\", \"part4\"]\n",
        "\n",
        "\n",
        "for song in songs: # loop over all songs\n",
        "\n",
        "  time_df = pd.read_csv ('/content/drive/MyDrive/Thesis/Experiment Dances/{}_frames.csv'.format(song))\n",
        "\n",
        "  frame_list = []\n",
        "\n",
        "  for index, row in time_df.iterrows():\n",
        "    start_frame = row['start_frame']\n",
        "    end_frame = row['end_frame']\n",
        "    for frame in range(start_frame, end_frame, frame_rate):\n",
        "      frame_list.append(frame)\n",
        "\n",
        "  for participant in participants: # loop over all participants\n",
        "\n",
        "    co_df = pd.DataFrame(columns = landmarks)\n",
        "    \n",
        "    vid = Video(\"/content/drive/MyDrive/Thesis/Experiment Dances/{}_{}.mov\".format(song, participant))\n",
        "    print('Processing {} - {}...'.format(song, participant))\n",
        "\n",
        "    # Run MediaPipe Pose and draw pose landmarks.\n",
        "    with mp_pose.Pose(\n",
        "        static_image_mode=False, min_detection_confidence=0.5, \n",
        "        model_complexity=2, enable_segmentation=True) as pose:\n",
        "\n",
        "      for frame_id in frame_list:\n",
        "        \n",
        "        frame = vid.get_frame(frame_id)\n",
        "        \n",
        "        # Convert the BGR image to RGB and process it with MediaPipe Pose:\n",
        "        results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        \n",
        "        image_hight, image_width, _ = frame.shape\n",
        "\n",
        "        c = []\n",
        "\n",
        "        if not results.pose_landmarks:\n",
        "          for j in range(0,33):\n",
        "            c.append((0, 0))\n",
        "        else:\n",
        "          for j in range(0,33):\n",
        "            c.append((results.pose_landmarks.landmark[j].x, results.pose_landmarks.landmark[j].y))\n",
        "        \n",
        "        try:\n",
        "          co_df.loc[frame_id] = c\n",
        "        except ValueError:\n",
        "          failed_frames.append(frame_id)\n",
        "\n",
        "    co_df.to_csv(\"/content/drive/MyDrive/Thesis/Experiment Dances/{}_{}_sk.csv\".format(song, participant))\n",
        "    print('Completed {} - {}!'.format(song, participant))"
      ],
      "metadata": {
        "id": "VSv1ZYmSFV_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the skeletal data from the baselines for every song:"
      ],
      "metadata": {
        "id": "4lMQYCJQ0PWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "\n",
        "mp_pose = mp.solutions.pose\n",
        "mp_drawing = mp.solutions.drawing_utils \n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "\n",
        "frame_rate = 2\n",
        "failed_frames = []\n",
        "\n",
        "landmarks = [\"nose\", \"left eye (inner)\", \"left eye\", \"left eye (outer)\", \"right eye (inner)\", \"right eye\", \"right eye (outer)\", \"left ear\", \"right ear\",\n",
        "             \"mouth (left)\", \"mouth (right)\", \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\", \"left wrist\", \"right wrist\", \"left pinky\",\n",
        "             \"right pinky\", \"left index\", \"right index\", \"left thumb\", \"right thumb\", \"left hip\", \"right hip\", \"left knee\", \"right knee\", \"left ankle\",\n",
        "             \"right ankle\", \"left heel\", \"right heel\", \"left foot index\", \"right foot index\"]\n",
        "\n",
        "songs = [\"americano\", \"diggin\", \"jagger\", \"pink\"] \n",
        "\n",
        "for song in songs: # loop over all songs\n",
        "\n",
        "  time_df = pd.read_csv ('/content/drive/MyDrive/Thesis/Experiment Dances/{}_frames.csv'.format(song))\n",
        "\n",
        "  frame_list = []\n",
        "\n",
        "  for index, row in time_df.iterrows():\n",
        "    start_frame = row['start_frame']\n",
        "    end_frame = row['end_frame']\n",
        "    for frame in range(start_frame, end_frame, frame_rate):\n",
        "      frame_list.append(frame)\n",
        "\n",
        "  co_df = pd.DataFrame(columns = landmarks)\n",
        "    \n",
        "  vid = Video(\"/content/drive/MyDrive/Thesis/Experiment Dances/{}_baseline.mp4\".format(song))\n",
        "  print('Processing {} - baseline...'.format(song))\n",
        "\n",
        "  # Run MediaPipe Pose and draw pose landmarks.\n",
        "  with mp_pose.Pose(\n",
        "      static_image_mode=False, min_detection_confidence=0.9, \n",
        "      model_complexity=0, enable_segmentation=True) as pose:\n",
        "    #for name, image in images.items():\n",
        "    for i in range(1):\n",
        "      # Convert the BGR image to RGB and process it with MediaPipe Pose.\n",
        "      results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "      \n",
        "      # Print nose landmark.\n",
        "      image_hight, image_width, _ = frame.shape\n",
        "      if not results.pose_landmarks:\n",
        "        continue\n",
        "\n",
        "      for j in range(0,33):\n",
        "        coordinates[j][0] = results.pose_landmarks.landmark[j].x #* image_width\n",
        "        coordinates[j][1] = results.pose_landmarks.landmark[j].y #* image_hight\n",
        "        #print(results.pose_landmarks.landmark[j].z)\n",
        "      #for pose_landmarks\n",
        "\n",
        "      # Draw pose landmarks.\n",
        "      #rint(f'Pose landmarks of {name}:')\n",
        "      annotated_image = frame.copy()\n",
        "      red_img = np.zeros_like(annotated_image, dtype=np.uint8)\n",
        "      red_img[:, :] = (255,255,255)\n",
        "      segm_2class = 0.2 + 0.8 * results.segmentation_mask\n",
        "      segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2)\n",
        "      annotated_image = annotated_image * segm_2class + red_img * (1 - segm_2class)\n",
        "      mp_drawing.draw_landmarks(\n",
        "          annotated_image,\n",
        "          results.pose_landmarks,\n",
        "          mp_pose.POSE_CONNECTIONS,\n",
        "          landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
        "      resize_and_show(annotated_image)"
      ],
      "metadata": {
        "id": "MbmLn52K0OrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the shapes of the dataframes:"
      ],
      "metadata": {
        "id": "Kqb0ypca1I6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "songs = [\"americano\", \"diggin\", \"jagger\", \"pink\"] \n",
        "\n",
        "participants.append('baseline')\n",
        "\n",
        "for song in songs:\n",
        "  print(song, ':')\n",
        "  for part in participants:\n",
        "    try:\n",
        "      df = pd.read_csv ('/content/drive/MyDrive/Thesis/Experiment Dances/{}_{}_sk.csv'.format(song, part))\n",
        "      print('- {}: {}'.format(part, df.shape))\n",
        "    except:\n",
        "      continue\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "a9JJ3tVYN0h2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hJPfi0wSQLn3",
        "m-tHsXwMQbja",
        "JOLa4NpHQk5P",
        "LvRjk5uXQrtJ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}